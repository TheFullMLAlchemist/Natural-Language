{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Importing Libraries\nimport nltk\nimport unicodedata\nimport inflect\nimport re\nfrom nltk.corpus import stopwords\nimport string\nimport gc\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a91e35a221e55fb0be0cd06c0849a4574c4ed441",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df = pd.read_table(\"../input/exmachina/toxicity_annotated_comments.tsv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f963b597e64d4c466de04008f22549b7571a1641",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "125134894211876bfe45ec40b4776bece78f0df4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Preprocessing:\n\n# Removing all thee non ascii characters\ndef remove_non_ascii(words): \n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n\n# Replacing all with lower-cases\ndef to_lowercase(words):\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\n\n# Removing all the punctuations \ndef remove_punctuation(words):\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\n# Replacing all the numbers with words\ndef replace_numbers(words):\n    p = inflect.engine()\n    new_words = []\n    for word in words:\n        if word.isdigit():\n            new_word = p.number_to_words(word)\n            new_words.append(new_word)\n        else:\n            new_words.append(word)\n    return new_words\n\n# Removing all the stopwords\ndef remove_stopwords(words):\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\n\n# Stemming\ndef stem_words(words):\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\n\n# Lemmatizing verbs\ndef lemmatize_verbs(words):\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\n\n# Grouping all them in one function\ndef normalize(words):\n    words = remove_non_ascii(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    words = replace_numbers(words)\n    words = remove_stopwords(words)\n    return words\n\n# Removing repetitive punctuations\n\nconsequitivedots = re.compile(r'\\.{1,}')\nconsequitivecommas = re.compile(r'\\,{1,}')\nconsequitivequestionmarks = re.compile(r'\\?{1,}')\nconsequitiveexclaimations = re.compile(r'\\!{1,}')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49319dd0c2beacfecf5c6e2008785213cedad98b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Tokenization\nfor i in range(len(df)):\n    X = nltk.word_tokenize(df[\"comment\"][i])\n    df[\"comment\"][i] = normalize(X)\n    print(i)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "67e87bd00d2bc8977ca29a29992eb7b76a20ef41"
      },
      "cell_type": "code",
      "source": "# attaching\nfor i in range(len(df)):\n    df[\"comment\"][i] = ' '.join(map(str, df[\"comment\"][i]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aaa876b7c2cdcee63014e1319745f892f6230134",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "for j in range(len(df)):\n    df[\"comment\"][j] = consequitivedots.sub(' ', df[\"comment\"][j])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "faf1ee5ad098cc0ad9d314a29ea0f304cb27ae39"
      },
      "cell_type": "code",
      "source": "for j in range(len(df)):\n    df[\"comment\"][j] = consequitivecommas.sub(' ', df[\"comment\"][j])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "1c21d9ac36120b6c7855de14d8ab558e04472f69"
      },
      "cell_type": "code",
      "source": "for j in range(len(df)):\n    df[\"comment\"][j] = consequitivequestionmarks.sub(' ', df[\"comment\"][j])\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "961d4ab2d1d11f472c4710b56db869f0ec74825b"
      },
      "cell_type": "code",
      "source": "for j in range(len(df)):\n    df[\"comment\"][j] = consequitiveexclaimations.sub(' ', df[\"comment\"][j])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "539e37062fe5ae2afda2b12d2e57ebd8d3752bb8",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "    \n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ea6fb0dbe32d078c96de0e9259dce5efd6d997f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\n    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d33663fda27953b34bd45a744290b56e7d74f6c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# label encoding\n\nfrom sklearn.preprocessing import LabelEncoder\ndf['logged_in'] = LabelEncoder().fit_transform(df['logged_in'])\ndf['ns'] = LabelEncoder().fit_transform(df['ns'])\ndf['sample'] = LabelEncoder().fit_transform(df['sample'])\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7fcd8559a97a602929ebb64e23408add5e157881",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\n# removing empty comments\ndf = df[df[\"comment\"] != '']\n# reseting index\ndf = df.reset_index()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "148a5b1157ee635c4b4b0826f1c2f331222a0fef"
      },
      "cell_type": "code",
      "source": "# dropping index column\ndf = df.drop('index', 1)\n\n# Sanity Check\ndf.head()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e699f69c088b94f209d1e1e67c87a845e020d30e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Saving into csv file\npreprocc = pd.DataFrame(df)\npreprocc.to_csv('preprocessed.csv', index=False)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "70a67addfb07b459664b15b481ef655706587203",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "5cf9b9eaa2ecbcf3dfd07edc6121261bd6e9667a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "6207dfbf094524abb0b297e28a8d7d3480ced1c1"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f7a240d0a1c419355f2ea62688dee4b94806ace7"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "e1d279be3e4a732c96f2596c6be00c2c0d7eb445"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "9f3dfa1b9ead46f5c38093ec95caec552e122eda"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "6e352f16de0c5ae53137077371d3cf2800dd49d9"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f9fff462da25dcd3ea2bf4dbeec6cf9e9bbd359",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# for i in range(len(df)):\n#     Y = ' '.join(df[\"Comment\"][i])\n#     df[\"Comment\"][i] = Y\n    \n# df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "534895d20eebe2208c8ee4dad18afcc0a33f58d1"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2fb2f81cabafa1e26355794f2b2762690fe0ee7b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# preprocc = pd.DataFrame(df)\n# preprocc.to_csv('train_preprocessed.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d119d8762e1fa2c021d05307252a1ffe2183b6d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# df=pd.read_csv(\"../input/coling/agr_en_train.csv\")\n# #df=pd.read_csv(\"dev.csv\")\n\n# df.columns=['id','text','tag']\n# #easier nonascii removal\n# df['text'] = df['text'].apply(lambda x: ''.join([\" \" if ord(i) < 32 or ord(i) > 126 else i for i in x]))\n# df['text']=df['text'].map(lambda x: x.lower())\n# df['text'] = df['text'].str.replace('[^\\w\\s]','')\n\n# #this may take time so i have comented\n# # df['text']=df['text'].astype(str).apply(lambda row: re.sub(r'(?<!\\S)\\d+(?!\\S)', lambda x: p.number_to_words(x.group()), row))\n\n# from nltk.corpus import stopwords\n# stop = stopwords.words('english')\n# #to have both versions in dataframe\n# df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n# #i dont like stemming that much\n\n# from nltk.stem import SnowballStemmer\n# stemmer = SnowballStemmer('english')\n# df['text'] = df['text'].apply(lambda x: ' '.join([stemmer.stem(token) for token in x.split()]))                                       \n\n# def consecutive(text):\n#     text = re.compile(r'\\.{1,}').sub(' ', text)\n#     text = re.compile(r'\\,{1,}').sub(' ', text)\n#     text = re.compile(r'\\?{1,}').sub(' ', text)\n#     text = re.compile(r'\\!{1,}').sub(' ', text)\n#     return text\n\n# from nltk.stem import WordNetLemmatizer\n# lemma = WordNetLemmatizer()\n# df['text'] = df['text'].apply(lambda x: ' '.join([lemma.lemmatize(word) for word in x.split()]))                                       \n# df['text'] =df['text'].apply(consecutive)\n\n\n\n# df.to_csv('train_preproc.csv')\n# #df.to_csv('dev_preproc.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "47b18e05c6b6f7e1a40c35783b96843878bab42e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# train_df = pd.read_csv('../input/preprocessed-csv/preprocessed.csv')\n# train_df = train_df.dropna()\n# train_df = train_df.reset_index()\n# from sklearn.preprocessing import LabelEncoder\n# train_df['Agg_Level'] = LabelEncoder().fit_transform(train_df['Agg_Level'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef70f6f79be34786202e1bda573f3d589e5177da",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# type(train_df['Comments'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "bea81cd47b5562858c65a82569e0c6360d30c8ec"
      },
      "cell_type": "code",
      "source": "# from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n# from sklearn.metrics import make_scorer, f1_score, precision_score, accuracy_score, log_loss\n# f1_scorer = make_scorer(f1_score, average=\"macro\")\n# precision_scorer = make_scorer(precision_score, average=\"macro\")\n# accuracy_scorer = make_scorer(accuracy_score, average=\"macro\")\n# log_loss_scorer = make_scorer(log_loss)\n\n# from sklearn.metrics import classification_report, confusion_matrix\n# from sklearn.pipeline import Pipeline\n# from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.feature_extraction.text import TfidfTransformer",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ad3bc177d4453c0b03da9065442dee150699c4a4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# X = train_df.iloc[:, 2].values\n# y = train_df.iloc[:, 1].values\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ad4fb00839732f64ea71336fcc6efee8b95be2c7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "34ff209b19e2771372ce1c79e83fc28c60a78d7f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# random_state_number = 967898\n# x_train, x_test, y_train, y_test = train_test_split(X, y,\n#                                                    test_size=0.10, random_state=random_state_number)\n\n# print(x_train.shape, y_train.shape)\n# print(x_test.shape, y_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "df283bbb7d9dff4c8ac382b4207d00363d06ee6e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# from sklearn.feature_extraction.text import CountVectorizer\n# count_vect = CountVectorizer()\n# X_train_counts = count_vect.fit_transform(X)\n# X_train_counts.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7e4b5afe4ffe91cae4a737ec871a0c8c2a5a4a7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# from sklearn.feature_extraction.text import TfidfTransformer\n# tfidf_transformer = TfidfTransformer()\n# X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n# X_train_tfidf.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be0169487775c46f946a2da0344068a8d7915ed0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# cvec = CountVectorizer(stop_words='english', min_df=1, max_df=.5, ngram_range=(1,2))\n# tfidf = TfidfTransformer()\n# cvec\n# cvec.fit(X)\n# cvec.fit(X)\n# len(cvec.vocabulary_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10798aa73f88a674a1c1eada899f76e1f5897b69",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# x_train_counts = cvec.fit_transform(x_train, y_train)\n# print(x_train_counts.shape)\n# x_train_tf = tfidf.fit_transform(x_train_counts)\n# print(x_train_tf.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "71de6a8dffd681bcbb0566f5440444f28eadce9b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# x_test_counts = cvec.fit_transform(x_test, y_test)\n# print(x_test_counts.shape)\n# x_test_tf = tfidf.fit_transform(x_test_counts)\n# print(x_test_tf.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5472ef565248df77e69425f35e1a7b33b30cc291",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# len(x_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f72fac3f5f192ed4ca496b613d1ddcd6f0c6c99b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# gc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "baab788da9cb82b6b1a6493fc9d5a20ab2723de0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# from sklearn.naive_bayes import MultinomialNB\n\n# predicted_prob = MultinomialNB(alpha=0.001).fit(x_test, y_test)\n# print(\"log_loss\\n\", log_loss(y_test, predicted_prob, labels=range(1,10)))\n        \n# y_pred = model.predict(x_test)\n# print(\"f1_score\\n\", f1_score(y_test, y_pred, average=\"macro\"))\n# print(\"accuracy_score\\n\", accuracy_score(y_test, y_pred))\n# print(\"\\nclassification_report\\n\",classification_report(y_test, y_pred))\n# print(\"\\nconfusion_matrix\\n\",confusion_matrix(y_test, y_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed1da7cd596b9a4635882ea34e9fb08530c67e83",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# type(x_train_tf)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "1f85d58025261d5699d35dee6a89281375817d5b"
      },
      "cell_type": "code",
      "source": "# from sklearn.naive_bayes import MultinomialNB\n# nb_model = MultinomialNB(alpha=0.001)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9314d578ff3f5ad02cf30639520ec87553c199c3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "7059e105d7d82d08e849b9b0af0c1e5f41d15b3a"
      },
      "cell_type": "code",
      "source": "# # Preprocessing:\n\n# # Removing all thee non ascii characters\n# def remove_non_ascii(words): \n#     new_words = []\n#     for word in words:\n#         new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n#         new_words.append(new_word)\n#     return new_words\n\n# # Replacing all with lower-cases\n# def to_lowercase(words):\n#     new_words = []\n#     for word in words:\n#         new_word = word.lower()\n#         new_words.append(new_word)\n#     return new_words\n\n# # Removing all the punctuations \n# def remove_punctuation(words):\n#     new_words = []\n#     for word in words:\n#         new_word = re.sub(r'[^\\w\\s]', '', word)\n#         if new_word != '':\n#             new_words.append(new_word)\n#     return new_words\n\n# # Replacing all the numbers with words\n# def replace_numbers(words):\n#     p = inflect.engine()\n#     new_words = []\n#     for word in words:\n#         if word.isdigit():\n#             new_word = p.number_to_words(word)\n#             new_words.append(new_word)\n#         else:\n#             new_words.append(word)\n#     return new_words\n\n# # Removing all the stopwords\n# def remove_stopwords(words):\n#     new_words = []\n#     for word in words:\n#         if word not in stopwords.words('english'):\n#             new_words.append(word)\n#     return new_words\n\n# # Stemming\n# def stem_words(words):\n#     stemmer = LancasterStemmer()\n#     stems = []\n#     for word in words:\n#         stem = stemmer.stem(word)\n#         stems.append(stem)\n#     return stems\n\n# # Lemmatizing verbs\n# def lemmatize_verbs(words):\n#     lemmatizer = WordNetLemmatizer()\n#     lemmas = []\n#     for word in words:\n#         lemma = lemmatizer.lemmatize(word, pos='v')\n#         lemmas.append(lemma)\n#     return lemmas\n\n# # Grouping all them in one function\n# def normalize(words):\n#     words = remove_non_ascii(words)\n#     words = to_lowercase(words)\n#     words = remove_punctuation(words)\n#     words = replace_numbers(words)\n#     words = remove_stopwords(words)\n#     return words\n\n# train = df[:10000]\n# dev = df[10000:]\n# train_labels = train[\"Agg_Level\"]\n# dev_labels = dev[\"Agg_Level\"]\n# dev\n\n# train_df = []\n# dev_df = []\n# train_words = []\n# dev_words = []\n# for i in range(10000):\n#     train_df.append(train[\"Comment\"][i])\n# for i in range(10000, 12000):\n#     dev_df.append(dev[\"Comment\"][i])\n\n# consequitivedots = re.compile(r'\\.{1,}')\n# consequitivecommas = re.compile(r'\\,{1,}')\n# consequitivequestionmarks = re.compile(r'\\?{1,}')\n# consequitiveexclaimations = re.compile(r'\\!{1,}')\n\n# for j in range(10000):\n#     train_df[j] = consequitivedots.sub(' ', train_df[j])\n#     train_df[j] = consequitivecommas.sub(' ', train_df[j])\n#     train_df[j] = consequitivequestionmarks.sub(' ', train_df[j])\n#     train_df[j] = consequitiveexclaimations.sub(' ', train_df[j])\n# for j in range(2000):\n#     dev_df[j] = consequitivedots.sub(' ', dev_df[j])\n#     dev_df[j] = consequitivecommas.sub(' ', dev_df[j])\n#     dev_df[j] = consequitivequestionmarks.sub(' ', dev_df[j])\n#     dev_df[j] = consequitiveexclaimations.sub(' ', dev_df[j])\n    \n    \n# # Tokenization\n# for i in range(10000):\n#     train_words.append(nltk.word_tokenize(train_df[i]))\n#     train_words[i] = normalize(train_words[i])\n\n# for i in range(len(dev_df)):\n#     dev_words.append(nltk.word_tokenize(dev_df[i]))\n#     dev_words[i] = normalize(dev_words[i])\n\n# preprocc = pd.DataFrame(train_words)\n# preprocc.to_csv('train_preprocessed.csv', index=False)\n\n# train_words_str = []\n# dev_words_str = []\n# for i in range(len(train_words)):\n#     train_words_str.append(', '.join(train_words[i]))\n# for i in range(2000):\n#     dev_words_str.append(', '.join(dev_words[i]))\n# train_words_str\n# dev_words_str\n\n# preprocc = pd.DataFrame(train_words_str)\n# preprocc.to_csv('train_preprocessed.csv', index=False)\n\n# dev_preprocc = pd.DataFrame(dev_words_str)\n# dev_preprocc.to_csv('dev_preprocessed.csv', index=False)\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5682a38c03fe0fcc84922ecce2bba02e9d675a6c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# X = all_text_train_df.iloc[:, 0].values\n# y = all_text_train_df.iloc[:, 1].values\n\n# # Splitting the dataset into the Training set and Test set\n# from sklearn.cross_validation import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n# from sklearn.feature_extraction.text import CountVectorizer\n# count_vect = CountVectorizer()\n# X_train_counts = count_vect.fit_transform(X_train)\n# from sklearn.feature_extraction.text import TfidfTransformer\n# tfidf_transformer = TfidfTransformer()\n# X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n# from sklearn.naive_bayes import MultinomialNB\n# clf = MultinomialNB().fit(X_train_tfidf, y_train)\n# from sklearn.pipeline import Pipeline\n\n# text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n\n# text_clf = text_clf.fit(X_train, y_train)\n# predicted = text_clf.predict(X_test)\n# np.mean(predicted == y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "32d5bea88beb081bb591811e0ec324af057d7791",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# import numpy as np\n# import pandas as pd\n# import string\n# import re\n\n# df=pd.read_csv(\"../input/coling/agr_en_train.csv\")\n# #df=pd.read_csv(\"dev.csv\")\n\n# df.columns=['id','text','tag']\n# #easier nonascii removal\n# df['text'] = df['text'].apply(lambda x: ''.join([\" \" if ord(i) < 32 or ord(i) > 126 else i for i in x]))\n# df['text']=df['text'].map(lambda x: x.lower())\n# df['text'] = df['text'].str.replace('[^\\w\\s]','')\n\n# #this may take time so i have comented\n# # df['text']=df['text'].astype(str).apply(lambda row: re.sub(r'(?<!\\S)\\d+(?!\\S)', lambda x: p.number_to_words(x.group()), row))\n\n# from nltk.corpus import stopwords\n# stop = stopwords.words('english')\n# #to have both versions in dataframe\n# df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n# #i dont like stemming that much\n\n# from nltk.stem import SnowballStemmer\n# stemmer = SnowballStemmer('english')\n# df['text'] = df['text'].apply(lambda x: ' '.join([stemmer.stem(token) for token in x.split()]))                                       \n\n# def consecutive(text):\n#     text = re.compile(r'\\.{1,}').sub(' ', text)\n#     text = re.compile(r'\\,{1,}').sub(' ', text)\n#     text = re.compile(r'\\?{1,}').sub(' ', text)\n#     text = re.compile(r'\\!{1,}').sub(' ', text)\n#     return text\n\n# from nltk.stem import WordNetLemmatizer\n# lemma = WordNetLemmatizer()\n# df['text'] = df['text'].apply(lambda x: ' '.join([lemma.lemmatize(word) for word in x.split()]))                                       \n# df['text'] =df['text'].apply(consecutive)\n\n\n\n# df.to_csv('train_preproc.csv')\n# #df.to_csv('dev_preproc.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "e4a25f405e285f460153ae4ea706d56df64b48c7"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}