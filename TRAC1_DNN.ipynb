{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77a6852362ee9e2dd414882d5d4828f32690b9ff",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom collections import defaultdict\nfrom collections import Counter\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d2ab33b260e476af05437b112b0f08811ba09eca"
      },
      "cell_type": "code",
      "source": "df = pd.read_csv(\"../input/preprocessed-with-emoji/preprocessed_with_emoji.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f311922fd2cc6a7dcdbe9edca20d35d35f09d8dc",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "comment = []\nfor i in range(len(df)):\n    comment.append(df.Comment[i])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09c256c968af3856a66a391006d2deb8e68870e4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "546b1dfa4650e12554604f33adebe3eb4f103f19",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def load_data(max_size, min_occurrences=10):\n    \"\"\"\n    Load data from a text file and creates the numpy arrays\n    used by the autoencoder.\n    :return: a tuple (sentences, sizes, vocabulary).\n        sentences is a 2-d matrix padded with EOS\n        sizes is a 1-d array with each sentence size\n        vocabulary is a list of words positioned according to their indices\n    \"\"\"\n    sentences = []\n    sizes = []\n    longest_sent_size = 0\n    index = [0]  # hack -- use a mutable object to be\n                 # accessed inside the nested function\n                 # at first, 0 means padding/EOS\n\n    def on_new_word():\n        index[0] += 1\n        return index[0]\n    word_dict = defaultdict(on_new_word)\n\n    \n    for line in comment:\n            #line = line.decode('utf-8')\n            tokens = line.split()\n#             print(tokens)\n            sent_size = len(tokens)\n            #if sent_size > max_size:\n            #    continue\n            sentences.append([word_dict[token]\n                             for token in tokens])\n            sizes.append(sent_size)\n            if sent_size > longest_sent_size:\n                longest_sent_size = sent_size\n\n    reverse_word_dict = {v: k for k, v in word_dict.items()}\n    reverse_word_dict[0] = '</s>'\n    # we initialize the matrix now that we know the number of sentences\n    sentence_matrix = np.full((len(sentences), longest_sent_size),\n                              0, np.int32)\n\n    for i, sent in enumerate(sentences):\n        sentence_array = np.array(sent)\n        sentence_matrix[i, :sizes[i]] = sentence_array\n\n    # count occurrences of tokens on the remaining sentences\n    # counter: index -> num_occurences\n    counter = Counter(sentence_matrix.flat)\n\n    # 0 signs the EOS token, it should be counted once per sentence\n    counter[0] = len(sentence_matrix)\n\n    # these words will be replaced by the unk token\n    unk_words = [(w, counter[w]) for w in counter\n                 if counter[w] < min_occurrences]\n    unk_count = sum(item[1] for item in unk_words)\n    unk_index = len(counter)   # make the unknown index the last one\n    counter[unk_index] = unk_count\n    reverse_word_dict[unk_index] = '<unk>'\n\n    # now we sort word indices by frequency (this works better with some\n    # sampling techniques such as Noise Constrastive Estimation)\n    replacements = {}\n    word_list = []\n    for new_index, (old_index, count) in enumerate(counter.most_common()):\n        if count < min_occurrences:\n            # we can break the loop because the next ones\n            # have equal or lower counts\n            break\n\n        replacements[old_index] = new_index\n        word_list.append(reverse_word_dict[old_index])\n\n    new_unk_index = replacements[unk_index]\n    replacements_with_unk = defaultdict(lambda: new_unk_index,\n                                        replacements)\n    original_shape = sentence_matrix.shape\n    replaced = np.array([replacements_with_unk[w]\n                         for w in sentence_matrix.flat],\n                        dtype=np.int32)\n    sentence_matrix = replaced.reshape(original_shape)\n\n    sizes_array = np.array(sizes, dtype=np.int32)\n    return sentence_matrix, sizes_array, word_list\n\nx,y,z=load_data( 50, min_occurrences=10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ce6410a2f8d86ad51116e1b151d9b0d69dc1225",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(char_level=False)\ntweets_token=[]\ntexts = [\"The sun is shining in June!\",\"September is grey.\",\"Life is beautiful in August.\",\"I like it\",\"This and other things?\"]\ntokenizer.fit_on_texts(df.Comment)\nprint(tokenizer.word_index)\nfor i in range(1,len(df[\"Comment\"])+1):\n    b=tokenizer.texts_to_sequences([comment[i-1]])\n    tweets_token.append(b)\ntweets_tokens=[]\nfor i in range(1,len(df[\"Comment\"])+1):\n    tweets_tokens.append(tweets_token[i-1][0])\n    \nlen(tweets_tokens)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee1635dc507497b466e6e9b4fa2842058c2ac0f5",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from numpy import array\nfrom numpy import argmax\nfrom keras.utils import to_categorical\n# define example\ndata = df[\"Agg_Level\"]\ndata = array(data)\nprint(data)\n# one hot encode\nencoded = to_categorical(data)\nprint(encoded)\n# invert encoding\ninverted = argmax(encoded[0])\nencoded[0]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "249fed714cb073d639013f2ed6e5cc21880a80b2",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from __future__ import print_function\nimport numpy as np\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalAveragePooling1D\nfrom keras.datasets import imdb\n\n\nmax_features = 23909\n# cut texts after this number of words\n# (among top max_features most common words)\nmaxlen = 23909\nbatch_size = 500\nprint('Loading data...')\nx_train, y_train, x_test, y_test = tweets_tokens[:10000], encoded[:10000], x[10000:], encoded[10000:]\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nmodel = Sequential()\nmodel.add(Embedding(23909, 128, input_length=maxlen))\nmodel.add(GlobalAveragePooling1D())\n# model.add(Dropout(0.5))\nmodel.add(Dense(batch_size*2, activation='tanh'))\nmodel.add(Dense(batch_size*2, activation='tanh'))\nmodel.add(Dense(batch_size, activation='tanh'))\nmodel.add(Dense(batch_size, activation='tanh'))\nmodel.add(Dense(batch_size, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n\nprint('Train...')\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=10,\n          validation_data=[x_test, y_test])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "3bdc86d4d5e1ff5831b73e667b69082c8326bebc"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb66aa0762caa827a577cbd119d8b3abb4172616",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# words1=[]\n# import pip\n# #pip.main(['install', '--user', 'tweet-preprocessor'])\n# import sys\n# #!{sys.executable} -m pip install gensim\n# import numpy as np\n# def loadGloveModel(gloveFile):\n#     print (\"Loading Glove Model\")\n#     f = open(gloveFile,'r',encoding=\"utf8\")\n#     model = {}\n#     for line in f:\n#         splitLine = line.split()\n#         word = splitLine[0]\n#         words1.append(word)\n#         embedding = np.array([float(val) for val in splitLine[1:]])\n#         model[word] = embedding\n#     print (\"Done.\",len(model),\" words loaded!\")\n#     return model\n# model1=loadGloveModel(\"glove.twitter.27B.200d.txt\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f7c3d7f04babb7f5ff67ff9684d0288ba756778",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# import numpy as np\n# import pandas as pd\n# from collections import defaultdict\n# import re\n\n# from bs4 import BeautifulSoup\n\n# import sys\n# import os\n\n# os.environ['KERAS_BACKEND']='theano'\n\n# from keras.preprocessing import sequence\n# from keras.utils.np_utils import to_categorical\n\n# from keras.layers import Embedding\n# from keras.layers import Dense, Input, Flatten\n# from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n# from keras.models import Model\n\n# from keras import backend as K\n# from keras.engine.topology import Layer, InputSpec\n# from keras import initializers\n\n# # cut texts after this number of words\n# # (among top max_features most common words)\n# maxlen = 21206\n# batch_size = 32\n# EMBEDDING_DIM=21206\n\n# embedding_layer = Embedding(21206,\n#                             200,\n#                             weights=[embedding_matrix],\n#                             input_length=maxlen,\n#                             trainable=True)\n\n# print('Loading data...')\n# x_train, y_train, x_test, y_test = x[:10000], df[\"Agg_Level\"][:10000], x[10000:], df[\"Agg_Level\"][10000:]\n# print(len(x_train), 'train sequences')\n# print(len(x_test), 'test sequences')\n\n# print('Pad sequences (samples x time)')\n# x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n# x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n# print('x_train shape:', x_train.shape)\n# print('x_test shape:', x_test.shape)\n# y_train = np.array(y_train)\n# y_test = np.array(y_test)\n\n# embedding_matrix = np.zeros((21207, 200))\n# for  index, word in enumerate(z):\n#     if index > 21205:\n#         break\n#     else:\n#         if word in df.Comment:\n#             embedding_vector = model1[word]\n#             embedding_matrix[index] = embedding_vector\n            \n# print(embedding_matrix)      \n\n# class AttLayer(Layer):\n#     def __init__(self, **kwargs):\n#         self.init = initializers.get('normal')\n#         #self.input_spec = [InputSpec(ndim=3)]\n#         super(AttLayer, self).__init__(**kwargs)\n\n#     def build(self, input_shape):\n#         assert len(input_shape)==3\n#         #self.W = self.init((input_shape[-1],1))\n#         self.W = self.init((input_shape[-1],))\n#         #self.input_spec = [InputSpec(shape=input_shape)]\n#         self.trainable_weights = [self.W]\n#         super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n\n#     def call(self, x, mask=None):\n#         eij = K.tanh(K.dot(x, self.W))\n        \n#         ai = K.exp(eij)\n#         weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n        \n#         weighted_input = x*weights.dimshuffle(0,1,'x')\n#         return weighted_input.sum(axis=1)\n\n#     def get_output_shape_for(self, input_shape):\n#         return (input_shape[0], input_shape[-1])\n\n# sentence_input = Input(shape=(maxlen,))\n# embedded_sequences = embedding_layer(sentence_input)\n# l_lstm = Bidirectional(GRU(100, return_sequences=False))(embedded_sequences)\n# #l_dense = TimeDistributed(Dense(100))(l_lstm)\n# l_att = AttLayer()(l_dense)\n# sentEncoder = Model(sentence_input, l_att)\n\n# review_input = Input(shape=(maxlen), dtype='int32')\n# #review_encoder = TimeDistributed(sentEncoder)(review_input)\n# l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_input)\n# l_dense_sent = TimeDistributed(Dense(100))(l_lstm_sent)\n# l_att_sent = AttLayer()(l_dense_sent)\n# preds = Dense(1, activation='softmax')(l_att_sent)\n# model = Model(review_input, preds)\n\n# model.compile(loss='categorical_crossentropy',\n#               optimizer='rmsprop',\n#               metrics=['acc'])\n\n# print(\"model fitting - Hierachical attention network\")\n# model.fit(x_train, y_train, validation_data=(x_val, y_val),\n#           nb_epoch=10, batch_size=50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "d1f087b3deacb7c6379e8096739e410bff7e8e26"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "959bcc5a3b17485316c0c2f6a555fddd899cd1a5",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import urllib2\n\ndata = urllib2.urlopen(\"https://nlp.stanford.edu/projects/glove/glove.twitter.27B.zip\") # read only 20 000 chars\ndata = data.split(\"\\n\") # then split it into lines\n\nfor line in data:\n    print(line)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "dd5dfcc451fb564b82246d0086715385d880d6dc"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "collapsed": true,
        "_uuid": "f4e4db5561c02c67441529a9ab124aa5ff8ce61b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "c51e2219c47333452a63418307176197da8092c8"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "_uuid": "e7b832e787b55e23a18ce3a60934e3e037671d5b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "bc0a5dd95cd845c9bdad400fc4b0c40f47875aa7"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "4b78663efac96b45f28991697b3ac042d06eccfe"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "5b0ec6530525ffad8fb46dca0392d78a553b0986"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "7c3eef7ed9635eab15f2021126bb2ee328036729"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "ab98d96d12be3e7fb38368b31b948b19f19e2d8e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "32be5ca4ad84e163938d02863b9909d8230c9965"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "4fa5634f285e10eaa42d7315b0367986483d50e7"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "456f2829c6049748762a1f6c17b7b910db0ba8b4"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "89fea40ed244e159a3ef9fa8b1a1bb0bcc963517"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "35feadcd1da391856b3521b292f67f97a1ef8eec"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "480f49438336f7bd3f849e1325c26b0eaaa8e6cc"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "d0c4743e5514929e5108d67d5fdfc4846138c506"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "cf6f8a598e6acbebbc4a4a97c3adcb6dbf3b0c42"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "ff3e585fac28c80f23bb35be11226479287ac303"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "d0820b8ed569d8079e52901a7361af87e2d8855d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}